{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import model as model_\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from pdb import set_trace as st\n",
    "from logger import setup_logger\n",
    "\n",
    "def data_prep(data_dir, train_batch, test_batch, use_1517, logger):\n",
    "    data_x = []\n",
    "    data_y = []\n",
    "    for i, f in enumerate(listdir(data_dir)):\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Loaded file \"+str(i)+\" of \"+str(len(listdir(data_dir))))\n",
    "        data = np.load(join(data_dir, f))\n",
    "        label = f.split('_')[0].split('c')[1]\n",
    "        data_x.append(data)\n",
    "        data_y.append(int(label)-1)\n",
    "    data_x = np.array(data_x)\n",
    "    data_y = np.array(data_y)\n",
    "    xshape = data_x.shape\n",
    "    data_x = data_x.reshape((xshape[0], xshape[1], 1, xshape[2], xshape[3]))\n",
    "    \n",
    "    \n",
    "#     # New data form\n",
    "#     mag = data_x[:, 4,...] + 1e-7\n",
    "#     cos_ = data_x[:, 0,...] \n",
    "#     sin_ = data_x[:, 1,...] \n",
    "#     data_x[:, 0,...] = np.log(mag)\n",
    "#     data_x[:, 1,...] = cos_\n",
    "#     data_x[:, 2,...] = sin_\n",
    "#     data_x = data_x[:, :3,...]\n",
    "    \n",
    "    \n",
    "    # Old data form\n",
    "    data_x[:, 0,...] = np.arccos(data_x[:, 0, 0,...]).reshape(data_x[:, 0,...].shape)\n",
    "    data_x[:, 1,...] = data_x[:, 4,...]\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_set_11 = torch.utils.data.TensorDataset(torch.from_numpy(data_x).type(torch.FloatTensor), torch.from_numpy (data_y).type(torch.LongTensor))\n",
    "    train_idx, test_idx = index_split(use_1517, logger)\n",
    "    data_train = torch.utils.data.Subset(data_set_11,indices=train_idx)\n",
    "    data_test = torch.utils.data.Subset(data_set_11,indices=test_idx)\n",
    "    params_train = {'batch_size': train_batch,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 1}\n",
    "    params_val = {'batch_size': test_batch,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 1}\n",
    "    train_generator = torch.utils.data.DataLoader(dataset=data_train, **params_train)\n",
    "    test_generator = torch.utils.data.DataLoader(dataset=data_test, **params_val)\n",
    "    return train_generator, test_generator \n",
    "\n",
    "def index_split(use_1517, logger):\n",
    "    #Splitting method for our MSTAR data\n",
    "    #If use_1517 is True, use the 15/17 depression split\n",
    "    #If use_1517 is False, use the Seen/Unseen data split\n",
    "    \n",
    "    csv_path = './chipinfo.csv' \n",
    "    df = pd.read_csv(csv_path)\n",
    "    training = df.loc[df['depression'] == 17]\n",
    "    subclass_9 = training.loc[training['target_type'] != 'bmp2_tank']\n",
    "    subclass_8 = subclass_9.loc[subclass_9['target_type'] != 't72_tank'].index.values\n",
    "    class_1_train = np.array(training.loc[training['serial_num']=='c21'].index.values)\n",
    "    class_3_train = np.array(training.loc[training['serial_num']=='132'].index.values)\n",
    "    subclass = np.concatenate([subclass_8, class_1_train, class_3_train], axis=0)\n",
    "    training = training.index\n",
    "    testing = df.loc[df['depression'] == 15]\n",
    "    subclass_test9 = testing.loc[testing['target_type'] != 'bmp2_tank']\n",
    "    subclass_test8 = np.array(subclass_test9.loc[subclass_test9['target_type']=='t72_tank'].index.values)\n",
    "    class_1_test2 = np.array(testing.loc[testing['serial_num']=='9563'].index.values)\n",
    "    class_1_test3 = np.array(testing.loc[testing['serial_num']=='9566'].index.values)\n",
    "    class_3_test2 = np.array(testing.loc[testing['serial_num']=='812'].index.values)\n",
    "    class_3_test3 = np.array(testing.loc[testing['serial_num']=='s7'].index.values)\n",
    "    subclass_test = np.concatenate([subclass_test8, class_1_test2, class_1_test3, class_3_test2, class_3_test3], axis=0)\n",
    "    testing = np.array(testing.index.values)\n",
    "    \n",
    "    if use_1517:\n",
    "        logger.info(\"15/17 Split\")\n",
    "        return training, testing\n",
    "    else:\n",
    "        logger.info(\"Seen/Unseen Split\")\n",
    "        return subclass, subclass_test\n",
    "\n",
    "def test(model, device, test_loader, logger, epoch):\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    pred_all = np.array([[]]).reshape((0, 1))\n",
    "    real_all = np.array([[]]).reshape((0, 1))\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            targets = target.cpu().numpy()\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    print(\"Test Accuracy is: \"+str(100. * correct / len(test_loader.dataset)))\n",
    "    logger.info(\"Test-\"+str(epoch)+\": \"+str(100. * correct / len(test_loader.dataset)))\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch, logger):\n",
    "    train_acc = 0\n",
    "    train_loss = 0\n",
    "    for it,(local_batch, local_labels) in enumerate(train_loader):\n",
    "        batch = torch.tensor(local_batch, requires_grad=True).cuda()\n",
    "        labels = local_labels.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        out, ii = model(batch)\n",
    "        _, predicted = torch.max(out, 1)\n",
    "        total = labels.shape[0]\n",
    "        train_acc += (predicted == labels).sum().item()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(out, labels)\n",
    "        train_loss += loss \n",
    "        (loss + ii).backward()\n",
    "        optimizer.step()\n",
    "    print(\"#####EPOCH \"+str(epoch)+\"#####\")\n",
    "    print(\"Train accuracy is: \"+str(train_acc / len(train_loader.dataset)*100.))\n",
    "    print(\"Train loss is: \"+str((train_loss / len(train_loader.dataset)*100.).item()))\n",
    "    logger.info(\"Loss: \"+str((train_loss / len(train_loader.dataset)*100.).item())+ \" Train-\"+str(epoch)+\": \"+str(train_acc / len(train_loader.dataset)*100.))\n",
    "        \n",
    "def main():\n",
    "    #argparse settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MSTAR Example') #400 and 0.001\n",
    "    parser.add_argument('--batchsize', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for training (default: 400)')\n",
    "    parser.add_argument('--test_batchsize', type=int, default=400, metavar='N',\n",
    "                        help='input batch size for testing (default: 400)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, metavar='N',\n",
    "                        help='number of epochs to train (default: 100)')\n",
    "    parser.add_argument('--lr', type=float, default=0.015, metavar='LR',\n",
    "                        help='learning rate (default: 0.015)')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, metavar='M',\n",
    "                        help='Adam momentum (default: 0.9)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--data-dir', type=str, default=\"./data_polar\", metavar='N',\n",
    "                        help='where data is stored')\n",
    "    parser.add_argument('--use-pretrain', type=int, default=1, metavar='N',\n",
    "                        help='Use pretrained model or not')\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('./log')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # For model saving purposes, initializes as 0\n",
    "    # If accuracy higher than \"higher\" then saves the model\n",
    "    highest = 0\n",
    "    \n",
    "    # The actual path to save\n",
    "    save_path=None\n",
    "    \n",
    "    try:\n",
    "        os.mkdir('./save')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    logger = setup_logger('MSTAR logger')\n",
    "    use_1517 = True\n",
    "    args = parser.parse_args()\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(args.seed)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    \n",
    "    \n",
    "    batches = [200]\n",
    "    lrs = [0.02]\n",
    "    ws = [1, 0, 0.5, 5, 10]\n",
    "            \n",
    "    for w in ws:        \n",
    "        model = model_.shrinkage(5, w).cuda()\n",
    "        logger.info(model)\n",
    "        model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "        logger.info(\"#Model Parameters: \"+str(params))\n",
    "        logger.info(w)    \n",
    "    \n",
    "#     batches = [50, 100, 200, 400, 800]\n",
    "#     lrs = [0.01, 0.015, 0.03, 0.05, 0.005, 0.08, 0.1, 0.005, 0.001]\n",
    "    \n",
    "    \n",
    "        for i in batches:\n",
    "            train_loader, test_loader = data_prep(args.data_dir, i, args.test_batchsize, use_1517, logger)\n",
    "            logger.info(\"Batch Size: \"+str(args.batchsize))\n",
    "            for j in lrs:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=j, eps=1e-8, amsgrad=True)\n",
    "                logger.info(\"Learning Rate: \"+str(args.lr))\n",
    "                for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "                    acc=test(model, device, test_loader, logger, epoch)\n",
    "                    if acc > highest:\n",
    "                        if save_path is not None:\n",
    "                            try:\n",
    "                                os.remove(save_path+'.ckpt')\n",
    "                            except:\n",
    "                                pass\n",
    "                        highest = acc\n",
    "                        save_path = os.path.join('./save/', '[{acc}]-[{batch}]-[{learning_rate}]-11class-model-[{model}]'.format(acc = np.round(acc, 3), batch=i, learning_rate=j, model=use_1517))\n",
    "                        torch.save(model.state_dict(), save_path+'.ckpt')\n",
    "                        logger.info('Saved model checkpoints into {}...'.format(save_path))\n",
    "\n",
    "                    train(model, device, train_loader, optimizer, epoch, logger)\n",
    "\n",
    "\n",
    "                logger.info(\"########## NEW MODEL ###########\")\n",
    "                model = model_.shrinkage(5, w).cuda()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "       \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
